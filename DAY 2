ğŸ“… DAY-2 â€” Data Cleaning + SQL Integration
ğŸ¯ Todayâ€™s Goals

âœ” Clean raw data â†’ Create clean dataset
âœ” Add Data Quality checks
âœ” Store cleaned data in structured folders
âœ” Start SQL work for analysis
âœ” Update repo professional format

ğŸ§© Task-1 â€” Create Clean Layer Folder

Inside your project create:

/data/raw/        â†’ raw downloaded data
/data/clean/      â†’ cleaned dataset


Move your existing CSV file to:

ğŸ“Œ /data/raw/raw_tips_data.csv

ğŸ§© Task-2 â€” Write Data Cleaning Script

File: clean_data.py
Paste this real-time cleaning pipeline:

import pandas as pd

# Load raw data
df = pd.read_csv("data/raw/raw_tips_data.csv")

# Data Quality Check
print("Initial Data Shape:", df.shape)

# Remove duplicates
df = df.drop_duplicates()

# Handling missing values
df = df.dropna()  # For now, drop rows with nulls

# Standardize column names
df.columns = df.columns.str.lower().str.replace(" ", "_")

# Save Clean Data
df.to_csv("data/clean/clean_tips_data.csv", index=False)

print("Clean Data Saved Successfully!")
print("Final Data Shape:", df.shape)


Run it â†’ You must see:
âœ” data shape reduced if any duplicates or nulls
âœ” new CSV created â†’ /data/clean/clean_tips_data.csv

ğŸ§© Task-3 â€” Create SQL Queries

Folder: /sql/

File-1: table_create.sql

CREATE TABLE tips_clean (
    total_bill FLOAT,
    tip FLOAT,
    sex VARCHAR(10),
    smoker VARCHAR(10),
    day VARCHAR(10),
    time VARCHAR(10),
    size INT
);


File-2: basic_queries.sql

SELECT * FROM tips_clean LIMIT 10;

SELECT sex, AVG(tip) AS avg_tip
FROM tips_clean
GROUP BY sex;

SELECT day, SUM(total_bill) AS total_sales
FROM tips_clean
GROUP BY day
ORDER BY total_sales DESC;


Upload both .sql files to GitHub.

ğŸ§© Task-4 â€” Notebook Update

ğŸ“Œ Add new notebook:

notebooks/Day2_Data_Cleaning.ipynb


Include:
âœ” Before & after data shapes
âœ” Missing values count
âœ” At least 3 insights in Markdown

Example insights:

- Weekend has higher total sales than weekdays  
- Male customers give slightly higher tips  
- Dinner time revenue > Lunch time revenue  

ğŸ§© Task-5 â€” README Update

Add this section:

## Day-2 Work Summary
âœ” Cleaned raw data using Python (duplicates & nulls removed)
âœ” Stored cleaned data in /data/clean/
âœ” Created SQL queries to analyze cleaned dataset
âœ” Documented insights in Jupyter Notebook

ğŸ† Day-2 Deliverables Checklist
Task	Status
Clean layer folder added	â¬œ
clean_data.py completed	â¬œ
clean_tips_data.csv generated	â¬œ
SQL folder + 2 scripts	â¬œ
Day-2 Notebook added	â¬œ
README updated	â¬œ
Commit + Push to GitHub	â¬œ
ğŸ“Œ Once done â†’ You reply:

ğŸ‘‰ Day-2 Completed

Then I will:
âœ” Review your repo
âœ” Give Day-3 task: EDA + Visualizations + More SQL
âœ” Provide YouTube sources for real Data Engineering best practices
âœ” Enhance your project structure like 4-years experience portfolio

Letâ€™s build it step-by-step. You're doing amazing ğŸš€ğŸ”¥
